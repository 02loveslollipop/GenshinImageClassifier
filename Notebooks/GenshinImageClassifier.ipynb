{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Genshin Image Classifier\n",
        "a convolutional neural network (CNN) model to classify images of characters from the game Genshin Impact. The model is trained on a dataset of 20 classes, each representing characters from the game. The dataset is obtained from web scraping images from pixiv.net using it tag search feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzS86gzpqq0"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ryWTeSacpnBr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txr6fqmWq-_C"
      },
      "source": [
        "## Check available GPU devices and empty cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x_BoSgt8AQci"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk9xZIvNrCzK",
        "outputId": "3790f979-bc79-4a4b-cd08-1f65f508d183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU (UUID: GPU-fcd7eac7-b588-89f2-ce92-bd0a6029e522)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFRQVXafqydh"
      },
      "source": [
        "## Select GPU as device if available\n",
        "select the device for training, if GPU is available, select GPU, otherwise select CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nv7bSbsTqW6s"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # Select GPU if available, else CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr8JDEM8Gy-w",
        "outputId": "57fe6f0c-bf23-422c-ffd3-91bfad764559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQMiHNmpszA5"
      },
      "source": [
        "## Mount the Drive unit (Only for Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RychxD8TuSpu",
        "outputId": "5d2219f5-e9c6-4f04-d46b-903a917b2b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Mount Google Drive to access dataset\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn7zLTlisOIc"
      },
      "source": [
        "## Define custom torch dataset class\n",
        "the dataset class is used to load the data from the image folders, into a pytorch dataset object. This class is used to load the data into the model for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WB3Ub6A1sVOw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "import os\n",
        "from PIL import Image\n",
        "     \n",
        "class GenshinDataSet(Dataset):\n",
        "    \n",
        "    class LabelDecoder:\n",
        "        def __init__(self, characterLabel: dict) -> None:\n",
        "            self.characterLabel = characterLabel # Dictionary to map character names to numerical labels\n",
        "        def __call__(self, label: int) -> str:\n",
        "            for character, characterLabel in self.characterLabel.items():\n",
        "                if characterLabel == label:\n",
        "                    return character\n",
        "            return None\n",
        "\n",
        "        def inverseTransform(self, label: torch.Tensor) -> str:\n",
        "            return self(label.item())\n",
        "        \n",
        "    \n",
        "    def __init__(self, directory: str, transforms: Compose = None) -> None:\n",
        "        self.directory = directory  # path to the dataset directory\n",
        "        self.characters = os.listdir(directory)  # List of characters as folder names\n",
        "        self.transforms = transforms  # Image transformations\n",
        "        self.images = []  # List of image paths\n",
        "        self.labels = []  # List of labels (numerical)\n",
        "        self.chararecterLabel = {}  # Dictionary to map character names to numerical labels\n",
        "        for character in self.characters:  # Loop through the list of characters to get the images and labels\n",
        "            \n",
        "            category_path = os.path.join(directory, character)  # Path to the character folder\n",
        "            label = len(self.characters) - 1 - self.characters.index(character)  # Assign numerical label based on character index\n",
        "            self.chararecterLabel[character] = label  # Map character name to numerical label\n",
        "\n",
        "            for image_file in os.listdir(category_path):  # Loop through the images in the character folder\n",
        "                image_path = os.path.join(category_path, image_file)\n",
        "                self.images.append(image_path)  # Append image path           \n",
        "                self.labels.append(label)  # Append numerical label\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[any, torch.Tensor]:  # Get the image and label at the specified index\n",
        "        with Image.open(self.images[index]) as img:\n",
        "            image = img.copy()\n",
        "        label = self.labels[index]  # Get the label\n",
        "\n",
        "        if image.mode == 'L':  # Check for grayscale mode ('L')\n",
        "                    image = image.convert('RGB')  # Convert to RGB mode\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, torch.tensor(label)  # Return the image and label as a tuple (image, torch.Tensor)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)  # Return the number of images in the dataset\n",
        "\n",
        "    def getLabelCount(self) -> int:\n",
        "        return len(self.characters)  # Return the number of characters in the dataset\n",
        "\n",
        "    def getCharacterInverseTransform(self) -> LabelDecoder:\n",
        "        return self.LabelDecoder(self.chararecterLabel)  # Return the LabelDecoder object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFDHsMo5rrsN"
      },
      "source": [
        "## Create dataset with transforms\n",
        "Create the pytorch dataset class with transforms for the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Vul1vuy3V35g"
      },
      "outputs": [],
      "source": [
        "mean = torch.tensor([0.0189, 0.0177, 0.0192]) # Mean values for normalization, get from normalizationParameters.py\n",
        "std = torch.tensor([0.0098, 0.0097, 0.0094]) # Standard deviation values for normalization, get from normalizationParameters.py\n",
        "batch_size = 64 # Batch size (number of images to process at once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local training\n",
        "all_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=mean, std=std)\n",
        "                                     ])\n",
        "\n",
        "train_dataset = GenshinDataSet(directory = r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\processed_images', transforms = all_transforms) # Load the training dataset\n",
        "\n",
        "num_classes = train_dataset.getLabelCount() # Get the number of classes in the dataset\n",
        "\n",
        "test_dataset = GenshinDataSet(directory = r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\processed_images_test', transforms = all_transforms) # Load the testing dataset\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True) # Instantiate loader objects to facilitate processing\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True) # Instantiate loader objects to facilitate processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GV3Y93z8rUBK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\Katana GF66 11UC\\AppData\\Local\\Temp\\ipykernel_23856\\169898249.py\", line 6, in <module>\n",
            "    train_dataset = GenshinDataSet(directory = '/content/gdrive/MyDrive/GenshinImageClassifier/128classifier/processed_images/', transforms = all_transforms) # Load the training dataset\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Katana GF66 11UC\\AppData\\Local\\Temp\\ipykernel_23856\\2820496547.py\", line 10, in __init__\n",
            "    self.characters = os.listdir(directory)  # List of characters as folder names\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [WinError 3] The system cannot find the path specified: '/content/gdrive/MyDrive/GenshinImageClassifier/128classifier/processed_images/'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1454, in structured_traceback\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1345, in structured_traceback\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1192, in structured_traceback\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1133, in get_records\n",
            "  File \"c:\\Users\\Katana GF66 11UC\\.conda\\envs\\genshinimageclassifierCUDA\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1073, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\n",
            "OSError: [Errno 24] Too many open files: 'c:\\\\Users\\\\Katana GF66 11UC\\\\.conda\\\\envs\\\\genshinimageclassifierCUDA\\\\Lib\\\\site-packages\\\\pygments\\\\styles\\\\default.py'\n"
          ]
        }
      ],
      "source": [
        "# Google Colab training\n",
        "all_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=mean, std=std)\n",
        "                                     ])\n",
        "\n",
        "train_dataset = GenshinDataSet(directory = '/content/gdrive/MyDrive/GenshinImageClassifier/128classifier/processed_images/', transforms = all_transforms) # Load the training dataset\n",
        "\n",
        "num_classes = train_dataset.getLabelCount() # Get the number of classes in the dataset\n",
        "\n",
        "test_dataset = GenshinDataSet(directory = '/content/gdrive/MyDrive/GenshinImageClassifier/128classifier/processed_images_test/', transforms = all_transforms) # Load the testing dataset\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True) # Instantiate loader objects to facilitate processing\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True) # Instantiate loader objects to facilitate processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y065LWGRsR6v"
      },
      "source": [
        "## Create the ConvNet Class\n",
        "define the ConvNet class that represents the convolutional neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qg5oxIg-sRsa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#input is a 128x128 image with 3 channels (RGB)\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self,category_count):\n",
        "    super(ConvNet,self).__init__()\n",
        "    self.convolution_layer1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=5) # Convolution layer 1: 3 input channels (RGB), 64 output channels (64 filters), 5x5 kernel size, output size = (128-5)/1 + 1 = 124\n",
        "    self.convolution_layer2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=5) # Convolution layer 2: 64 input channels (64 kernels from previous layer), 64 output channels (64 filters), 5x5 kernel size, output size = (124-5)/1 + 1 = 120\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2) # Max pooling layer: 2x2 kernel size, stride 2 (reduces image size by 2, 120x120 to 60x60)\n",
        "    self.dropout = nn.Dropout2d(p=0.5) # Dropout layer: 50% dropout rate\n",
        "\n",
        "    self.convolution_layer3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3) # Convolution layer 3: 64 input channels (64 kernels from previous layer), 128 output channels (128 filters), 3x3 kernel size, output size = (60-3)/1 + 1 = 58\n",
        "    self.convolution_layer4 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3) # Convolution layer 4: 128 input channels (128 kernels from previous layer), 128 output channels (128 filters), 3x3 kernel size, output size = (58-3)/1 + 1 = 56\n",
        "    self.max_pool2 = nn.MaxPool2d(kernel_size=2,stride=2) # Max pooling layer 2: 2x2 kernel size, stride 2 (reduces image size by 2, 64x64 to 32x32), output size = 56/2 = 28\n",
        "    \n",
        "    self.convolution_layer5 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3) # Convolution layer 5: 128 input channels (128 kernels from previous layer), 256 output channels (256 filters), 3x3 kernel size, output size = (28-3)/1 + 1 = 26\n",
        "    self.convolution_layer6 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3) # Convolution layer 6: 256 input channels (256 kernels from previous layer), 256 output channels (256 filters), 3x3 kernel size, output size = (26-3)/1 + 1 = 24\n",
        "    self.max_pool3 = nn.MaxPool2d(kernel_size=2,stride=2) # Max pooling layer 3: 2x2 kernel size, stride 2 (reduces image size by 2, 32x32 to 16x16), output size = 24/2 = 12\n",
        "\n",
        "    self.fully_connected1 = nn.Linear(128*12*12,4096) # Fully connected layer 1: 128*12*12 input features (128 filters from last convolution layer, 12x12 image size), 128 output features\n",
        "    self.relu = nn.ReLU() # ReLU activation function\n",
        "    self.fully_connected2 = nn.Linear(4096,1024) # Fully connected layer 2: 4096 inputs 1024 outputs\n",
        "    self.relu2 = nn.ReLU() # ReLU activation function\n",
        "    self.fully_connected3 = nn.Linear(1024,128) # Fully connected layer 3: 1024 inputs 512 outputs\n",
        "    self.relu3 = nn.ReLU() # ReLU activation function\n",
        "    self.fully_connected4 = nn.Linear(128,category_count) # Output layer: linear layer\n",
        "    self.softmax = nn.Softmax(dim=1) # Softmax activation function\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.convolution_layer1(x) # Convolution layer 1\n",
        "    #tanh activation function\n",
        "    output = self.convolution_layer2(output) # Convolution layer 2\n",
        "    output = self.max_pool(output) # Max pooling layer\n",
        "    output = self.dropout(output) # Dropout layer\n",
        "\n",
        "    output = self.convolution_layer3(output) # Convolution layer 3\n",
        "    output = self.convolution_layer4(output) # Convolution layer 4\n",
        "    output = self.max_pool2(output) # Max pooling layer 2\n",
        "\n",
        "    output = self.convolution_layer5(output) # Convolution layer 5\n",
        "    output = self.convolution_layer6(output) # Convolution layer 6\n",
        "    output = self.max_pool3(output) # Max pooling layer 3\n",
        "\n",
        "    output = output.reshape(output.size(0),-1) # Flatten the output for the fully connected layer\n",
        "\n",
        "    output = self.fully_connected1(output) # Fully connected layer 1\n",
        "    output = self.relu(output)  # ReLU activation function\n",
        "    output = self.fully_connected2(output) # Fully connected layer 2\n",
        "    output = self.relu2(output) # ReLU activation function\n",
        "    output = self.fully_connected3(output) # Fully connected layer 3\n",
        "    output = self.relu3(output) # ReLU activation function\n",
        "    output = self.fully_connected4(output) # Output layer\n",
        "    output = self.softmax(output) # Softmax activation function\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hghM7VoMB-v9"
      },
      "source": [
        "## Hyperparameters configuration\n",
        "set epochs, learning_rate, num_classes, device, loss function, optimizer and total steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8CNOAXoeCC-L"
      },
      "outputs": [],
      "source": [
        "size = 128 # Image resolution for the model input\n",
        "num_epochs = 10 # Number of epochs for training\n",
        "\n",
        "model = ConvNet(num_classes) # Create the model with the number of classes required\n",
        "\n",
        "lossFunction = nn.CrossEntropyLoss() # Set loss function as CrossEntropyLoss\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay = 0.005, momentum = 0.9) # Set thr optimizer as a Stochastic Gradient Descent with the learning rate, weight decay and momentum\n",
        "\n",
        "total_step = len(train_loader) # Set the total step as the length of the train loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6wGaOyDC9_"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYCxi2-QCy9J",
        "outputId": "3a84404b-b750-4e6d-ee36-b51623aa3176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.7877992391586304\n",
            "loss: 1.7894737720489502\n",
            "loss: 1.7938601970672607\n",
            "loss: 1.7908748388290405\n",
            "loss: 1.7920187711715698\n",
            "loss: 1.787894368171692\n",
            "loss: 1.786717176437378\n",
            "loss: 1.7905832529067993\n",
            "loss: 1.7876291275024414\n",
            "loss: 1.7908002138137817\n",
            "loss: 1.789919137954712\n",
            "loss: 1.789108395576477\n",
            "loss: 1.7938097715377808\n",
            "loss: 1.7857593297958374\n",
            "loss: 1.7946889400482178\n",
            "loss: 1.7911832332611084\n",
            "loss: 1.7926805019378662\n",
            "loss: 1.7935559749603271\n",
            "loss: 1.7926223278045654\n",
            "loss: 1.788307547569275\n",
            "loss: 1.789299726486206\n",
            "loss: 1.7918493747711182\n",
            "loss: 1.7939385175704956\n",
            "loss: 1.787717342376709\n",
            "loss: 1.7905693054199219\n",
            "loss: 1.7893366813659668\n",
            "loss: 1.7825335264205933\n",
            "loss: 1.7878518104553223\n",
            "loss: 1.7873337268829346\n",
            "loss: 1.786942958831787\n",
            "loss: 1.7877390384674072\n",
            "loss: 1.7826255559921265\n",
            "loss: 1.781324863433838\n",
            "loss: 1.7871679067611694\n",
            "loss: 1.7971539497375488\n",
            "loss: 1.7978023290634155\n",
            "loss: 1.7948113679885864\n",
            "loss: 1.784878134727478\n",
            "loss: 1.7912105321884155\n",
            "loss: 1.7968083620071411\n",
            "loss: 1.7838482856750488\n",
            "loss: 1.7913836240768433\n",
            "loss: 1.7841628789901733\n",
            "loss: 1.7895110845565796\n",
            "loss: 1.7864391803741455\n",
            "loss: 1.7887707948684692\n",
            "loss: 1.7824941873550415\n",
            "loss: 1.7888498306274414\n",
            "loss: 1.7952615022659302\n",
            "loss: 1.798317313194275\n",
            "loss: 1.785330891609192\n",
            "loss: 1.781479001045227\n",
            "loss: 1.778256893157959\n",
            "loss: 1.7772753238677979\n",
            "loss: 1.784763216972351\n",
            "loss: 1.7925286293029785\n",
            "loss: 1.777794599533081\n",
            "loss: 1.7864396572113037\n",
            "loss: 1.7862956523895264\n",
            "loss: 1.7884531021118164\n",
            "loss: 1.782889485359192\n",
            "loss: 1.778568983078003\n",
            "loss: 1.788472294807434\n",
            "loss: 1.7774055004119873\n",
            "loss: 1.786489486694336\n",
            "loss: 1.7855618000030518\n",
            "loss: 1.7936328649520874\n",
            "loss: 1.7846328020095825\n",
            "loss: 1.7748788595199585\n",
            "loss: 1.7886327505111694\n",
            "loss: 1.7655762434005737\n",
            "loss: 1.7743474245071411\n",
            "loss: 1.790550708770752\n",
            "loss: 1.7767512798309326\n",
            "loss: 1.794160008430481\n",
            "loss: 1.7743810415267944\n",
            "loss: 1.7887794971466064\n",
            "loss: 1.7654212713241577\n",
            "loss: 1.7880988121032715\n",
            "loss: 1.7581419944763184\n",
            "loss: 1.797592043876648\n",
            "loss: 1.7970232963562012\n",
            "loss: 1.7877825498580933\n",
            "loss: 1.800879716873169\n",
            "loss: 1.779172658920288\n",
            "loss: 1.7650295495986938\n",
            "loss: 1.8131651878356934\n",
            "loss: 1.7964305877685547\n",
            "loss: 1.8122835159301758\n",
            "loss: 1.8022104501724243\n",
            "loss: 1.7868484258651733\n",
            "loss: 1.7832889556884766\n",
            "loss: 1.7857186794281006\n",
            "loss: 1.7851905822753906\n",
            "loss: 1.7781366109848022\n",
            "loss: 1.7945276498794556\n",
            "loss: 1.7723249197006226\n",
            "loss: 1.783582329750061\n",
            "loss: 1.7773997783660889\n",
            "loss: 1.7901251316070557\n",
            "loss: 1.7880728244781494\n",
            "loss: 1.7834618091583252\n",
            "loss: 1.7935829162597656\n",
            "loss: 1.7864816188812256\n",
            "loss: 1.7798110246658325\n",
            "loss: 1.7795501947402954\n",
            "loss: 1.7875851392745972\n",
            "loss: 1.7887202501296997\n",
            "loss: 1.7737526893615723\n",
            "loss: 1.7907990217208862\n",
            "loss: 1.7678489685058594\n",
            "loss: 1.762350082397461\n",
            "loss: 1.781119704246521\n",
            "loss: 1.7672163248062134\n",
            "loss: 1.7913057804107666\n",
            "loss: 1.7664391994476318\n",
            "loss: 1.782889485359192\n",
            "loss: 1.768281102180481\n",
            "loss: 1.7729040384292603\n",
            "loss: 1.7990214824676514\n",
            "loss: 1.7776719331741333\n",
            "loss: 1.772373914718628\n",
            "loss: 1.7733724117279053\n",
            "loss: 1.7646009922027588\n",
            "loss: 1.774222493171692\n",
            "loss: 1.789321780204773\n",
            "loss: 1.7644718885421753\n",
            "loss: 1.7477953433990479\n",
            "loss: 1.7842570543289185\n",
            "loss: 1.768218755722046\n",
            "loss: 1.7526445388793945\n",
            "loss: 1.7827916145324707\n",
            "loss: 1.78285813331604\n",
            "loss: 1.7798547744750977\n",
            "loss: 1.783547282218933\n",
            "loss: 1.7815959453582764\n",
            "loss: 1.7805430889129639\n",
            "loss: 1.7623569965362549\n",
            "loss: 1.783035397529602\n",
            "loss: 1.7843989133834839\n",
            "loss: 1.7626099586486816\n",
            "loss: 1.7611359357833862\n",
            "loss: 1.7759923934936523\n",
            "loss: 1.7608954906463623\n",
            "loss: 1.7837284803390503\n",
            "loss: 1.7678331136703491\n",
            "loss: 1.7819581031799316\n",
            "loss: 1.7443348169326782\n",
            "loss: 1.7271018028259277\n",
            "loss: 1.7858686447143555\n",
            "Epoch [1/10], Loss: 1.7859\n",
            "loss: 1.7653100490570068\n",
            "loss: 1.7946771383285522\n",
            "loss: 1.7394216060638428\n",
            "loss: 1.7577636241912842\n",
            "loss: 1.7867051362991333\n",
            "loss: 1.7424530982971191\n",
            "loss: 1.758470058441162\n",
            "loss: 1.7341169118881226\n",
            "loss: 1.743422508239746\n",
            "loss: 1.7712182998657227\n",
            "loss: 1.777060866355896\n",
            "loss: 1.7696568965911865\n",
            "loss: 1.7310150861740112\n",
            "loss: 1.7712757587432861\n",
            "loss: 1.7669744491577148\n",
            "loss: 1.7407591342926025\n",
            "loss: 1.7597630023956299\n",
            "loss: 1.7110700607299805\n",
            "loss: 1.7464962005615234\n",
            "loss: 1.7681370973587036\n",
            "loss: 1.7639316320419312\n",
            "loss: 1.7129402160644531\n",
            "loss: 1.7647976875305176\n",
            "loss: 1.7764568328857422\n",
            "loss: 1.7506015300750732\n",
            "loss: 1.7456514835357666\n",
            "loss: 1.7475147247314453\n",
            "loss: 1.75726318359375\n",
            "loss: 1.7379391193389893\n",
            "loss: 1.757064700126648\n",
            "loss: 1.7557469606399536\n",
            "loss: 1.7335768938064575\n",
            "loss: 1.740348219871521\n",
            "loss: 1.734114646911621\n",
            "loss: 1.7628885507583618\n",
            "loss: 1.7009018659591675\n",
            "loss: 1.7144315242767334\n",
            "loss: 1.6934046745300293\n",
            "loss: 1.7545466423034668\n",
            "loss: 1.7079365253448486\n",
            "loss: 1.7049617767333984\n",
            "loss: 1.7160096168518066\n",
            "loss: 1.7022933959960938\n",
            "loss: 1.719728946685791\n",
            "loss: 1.6994177103042603\n",
            "loss: 1.70295250415802\n",
            "loss: 1.7306939363479614\n",
            "loss: 1.794325828552246\n",
            "loss: 1.720672369003296\n",
            "loss: 1.7166500091552734\n",
            "loss: 1.7286127805709839\n",
            "loss: 1.72519052028656\n",
            "loss: 1.7323249578475952\n",
            "loss: 1.7194709777832031\n",
            "loss: 1.6890199184417725\n",
            "loss: 1.6771152019500732\n",
            "loss: 1.6817493438720703\n",
            "loss: 1.6962615251541138\n",
            "loss: 1.6961650848388672\n",
            "loss: 1.6767288446426392\n",
            "loss: 1.681931495666504\n",
            "loss: 1.6995056867599487\n",
            "loss: 1.7314270734786987\n",
            "loss: 1.65877103805542\n",
            "loss: 1.7199630737304688\n",
            "loss: 1.7012454271316528\n",
            "loss: 1.7267252206802368\n",
            "loss: 1.7444772720336914\n",
            "loss: 1.710676670074463\n",
            "loss: 1.7429344654083252\n",
            "loss: 1.741493582725525\n",
            "loss: 1.6584441661834717\n",
            "loss: 1.7608342170715332\n",
            "loss: 1.7583192586898804\n",
            "loss: 1.6975065469741821\n",
            "loss: 1.7354960441589355\n",
            "loss: 1.6818861961364746\n",
            "loss: 1.6771328449249268\n",
            "loss: 1.6959645748138428\n",
            "loss: 1.655341386795044\n",
            "loss: 1.6331850290298462\n",
            "loss: 1.6852591037750244\n",
            "loss: 1.7366580963134766\n",
            "loss: 1.6709587574005127\n",
            "loss: 1.6493439674377441\n",
            "loss: 1.6563701629638672\n",
            "loss: 1.6252127885818481\n",
            "loss: 1.7117775678634644\n",
            "loss: 1.676719069480896\n",
            "loss: 1.6377733945846558\n",
            "loss: 1.643030047416687\n",
            "loss: 1.672609567642212\n",
            "loss: 1.6022789478302002\n",
            "loss: 1.6801279783248901\n",
            "loss: 1.6689558029174805\n",
            "loss: 1.7193934917449951\n",
            "loss: 1.6859019994735718\n",
            "loss: 1.6142773628234863\n",
            "loss: 1.6865218877792358\n",
            "loss: 1.6327953338623047\n",
            "loss: 1.6754220724105835\n",
            "loss: 1.7660772800445557\n",
            "loss: 1.6341527700424194\n",
            "loss: 1.6462078094482422\n",
            "loss: 1.6627528667449951\n",
            "loss: 1.6277724504470825\n",
            "loss: 1.6829334497451782\n",
            "loss: 1.6045563220977783\n",
            "loss: 1.582634449005127\n",
            "loss: 1.7125276327133179\n",
            "loss: 1.6350730657577515\n",
            "loss: 1.5818836688995361\n",
            "loss: 1.6760733127593994\n",
            "loss: 1.6647624969482422\n",
            "loss: 1.628394365310669\n",
            "loss: 1.6148101091384888\n",
            "loss: 1.7378854751586914\n",
            "loss: 1.6242657899856567\n",
            "loss: 1.6792386770248413\n",
            "loss: 1.673769474029541\n",
            "loss: 1.6732717752456665\n",
            "loss: 1.7056884765625\n",
            "loss: 1.6375936269760132\n",
            "loss: 1.6820939779281616\n",
            "loss: 1.6591213941574097\n",
            "loss: 1.6815773248672485\n",
            "loss: 1.6514537334442139\n",
            "loss: 1.6948026418685913\n",
            "loss: 1.7773171663284302\n",
            "loss: 1.7656967639923096\n",
            "loss: 1.7921693325042725\n",
            "loss: 1.7469958066940308\n",
            "loss: 1.6775208711624146\n",
            "loss: 1.617898941040039\n",
            "loss: 1.6731921434402466\n",
            "loss: 1.6875488758087158\n",
            "loss: 1.6163009405136108\n",
            "loss: 1.7027292251586914\n",
            "loss: 1.7009854316711426\n",
            "loss: 1.6861586570739746\n",
            "loss: 1.6020272970199585\n",
            "loss: 1.6551125049591064\n",
            "loss: 1.6016888618469238\n",
            "loss: 1.6439801454544067\n",
            "loss: 1.6624268293380737\n",
            "loss: 1.6180769205093384\n",
            "loss: 1.733172059059143\n",
            "loss: 1.674216628074646\n",
            "loss: 1.6823347806930542\n",
            "loss: 1.7101515531539917\n",
            "Epoch [2/10], Loss: 1.7102\n",
            "loss: 1.5940775871276855\n",
            "loss: 1.69373619556427\n",
            "loss: 1.6756048202514648\n",
            "loss: 1.5576918125152588\n",
            "loss: 1.5893306732177734\n",
            "loss: 1.6447817087173462\n",
            "loss: 1.6898208856582642\n",
            "loss: 1.810912013053894\n",
            "loss: 1.7463759183883667\n",
            "loss: 1.6931960582733154\n",
            "loss: 1.5985112190246582\n",
            "loss: 1.6129873991012573\n",
            "loss: 1.6033176183700562\n",
            "loss: 1.5885894298553467\n",
            "loss: 1.6706453561782837\n",
            "loss: 1.6757664680480957\n",
            "loss: 1.6254653930664062\n",
            "loss: 1.6122266054153442\n",
            "loss: 1.6623084545135498\n",
            "loss: 1.5894330739974976\n",
            "loss: 1.5619745254516602\n",
            "loss: 1.6824800968170166\n",
            "loss: 1.6277344226837158\n",
            "loss: 1.6573166847229004\n",
            "loss: 1.6001592874526978\n",
            "loss: 1.7288120985031128\n",
            "loss: 1.669271469116211\n",
            "loss: 1.5424940586090088\n",
            "loss: 1.5863274335861206\n",
            "loss: 1.5856703519821167\n",
            "loss: 1.5830330848693848\n",
            "loss: 1.64461350440979\n",
            "loss: 1.629581093788147\n",
            "loss: 1.6073781251907349\n",
            "loss: 1.6674705743789673\n",
            "loss: 1.5863871574401855\n",
            "loss: 1.5054960250854492\n",
            "loss: 1.6973090171813965\n",
            "loss: 1.6220329999923706\n",
            "loss: 1.6684845685958862\n",
            "loss: 1.5661143064498901\n",
            "loss: 1.552988052368164\n",
            "loss: 1.5324915647506714\n",
            "loss: 1.6249566078186035\n",
            "loss: 1.641066551208496\n",
            "loss: 1.64203941822052\n",
            "loss: 1.6010191440582275\n",
            "loss: 1.536987066268921\n",
            "loss: 1.5906038284301758\n",
            "loss: 1.6149221658706665\n",
            "loss: 1.679852843284607\n",
            "loss: 1.645035982131958\n",
            "loss: 1.586445927619934\n",
            "loss: 1.651628851890564\n",
            "loss: 1.6180684566497803\n",
            "loss: 1.562282919883728\n",
            "loss: 1.5460560321807861\n",
            "loss: 1.6062812805175781\n",
            "loss: 1.5854315757751465\n",
            "loss: 1.6423486471176147\n",
            "loss: 1.5038293600082397\n",
            "loss: 1.6939815282821655\n",
            "loss: 1.7046864032745361\n",
            "loss: 1.5723167657852173\n",
            "loss: 1.5917268991470337\n",
            "loss: 1.5924410820007324\n",
            "loss: 1.6036912202835083\n",
            "loss: 1.6654574871063232\n",
            "loss: 1.6435096263885498\n",
            "loss: 1.599906325340271\n",
            "loss: 1.626879096031189\n",
            "loss: 1.6239376068115234\n",
            "loss: 1.5332149267196655\n",
            "loss: 1.6348472833633423\n",
            "loss: 1.5793046951293945\n",
            "loss: 1.5555884838104248\n",
            "loss: 1.5278998613357544\n",
            "loss: 1.61216139793396\n",
            "loss: 1.6374484300613403\n",
            "loss: 1.5875667333602905\n",
            "loss: 1.687491774559021\n",
            "loss: 1.575566291809082\n",
            "loss: 1.6639639139175415\n",
            "loss: 1.5793479681015015\n",
            "loss: 1.6249440908432007\n",
            "loss: 1.6686078310012817\n",
            "loss: 1.5833381414413452\n",
            "loss: 1.5117194652557373\n",
            "loss: 1.7148466110229492\n",
            "loss: 1.5832531452178955\n",
            "loss: 1.7234715223312378\n",
            "loss: 1.6037291288375854\n",
            "loss: 1.602988600730896\n",
            "loss: 1.5648159980773926\n",
            "loss: 1.6191521883010864\n",
            "loss: 1.5087547302246094\n",
            "loss: 1.6059824228286743\n",
            "loss: 1.4831761121749878\n",
            "loss: 1.6535460948944092\n",
            "loss: 1.5719839334487915\n",
            "loss: 1.583504557609558\n",
            "loss: 1.4813683032989502\n",
            "loss: 1.4792953729629517\n",
            "loss: 1.5405638217926025\n",
            "loss: 1.59105384349823\n",
            "loss: 1.6651852130889893\n",
            "loss: 1.6161836385726929\n",
            "loss: 1.5296326875686646\n",
            "loss: 1.6749869585037231\n",
            "loss: 1.6866339445114136\n",
            "loss: 1.6242585182189941\n",
            "loss: 1.588159203529358\n",
            "loss: 1.565548062324524\n",
            "loss: 1.533919095993042\n",
            "loss: 1.576127290725708\n",
            "loss: 1.5886588096618652\n",
            "loss: 1.5975093841552734\n",
            "loss: 1.565152883529663\n",
            "loss: 1.5979596376419067\n",
            "loss: 1.4952794313430786\n",
            "loss: 1.6291526556015015\n",
            "loss: 1.553961157798767\n",
            "loss: 1.5377569198608398\n",
            "loss: 1.5707451105117798\n",
            "loss: 1.6405636072158813\n",
            "loss: 1.5541712045669556\n",
            "loss: 1.624227523803711\n",
            "loss: 1.5950515270233154\n",
            "loss: 1.5628035068511963\n",
            "loss: 1.5452356338500977\n",
            "loss: 1.6302765607833862\n",
            "loss: 1.5964632034301758\n",
            "loss: 1.613013744354248\n",
            "loss: 1.539749264717102\n",
            "loss: 1.642912745475769\n",
            "loss: 1.49854576587677\n",
            "loss: 1.6185500621795654\n",
            "loss: 1.641443133354187\n",
            "loss: 1.5139522552490234\n",
            "loss: 1.6143332719802856\n",
            "loss: 1.5336588621139526\n",
            "loss: 1.5273686647415161\n",
            "loss: 1.5614005327224731\n",
            "loss: 1.644200086593628\n",
            "loss: 1.5475513935089111\n",
            "loss: 1.5412287712097168\n",
            "loss: 1.52009916305542\n",
            "loss: 1.5728342533111572\n",
            "loss: 1.6883654594421387\n",
            "loss: 1.5148175954818726\n",
            "Epoch [3/10], Loss: 1.5148\n",
            "loss: 1.5653504133224487\n",
            "loss: 1.5144805908203125\n",
            "loss: 1.5123319625854492\n",
            "loss: 1.5912188291549683\n",
            "loss: 1.494284749031067\n",
            "loss: 1.529226303100586\n",
            "loss: 1.5175540447235107\n",
            "loss: 1.609782099723816\n",
            "loss: 1.5397297143936157\n",
            "loss: 1.4824644327163696\n",
            "loss: 1.5623060464859009\n",
            "loss: 1.6498264074325562\n",
            "loss: 1.6322115659713745\n",
            "loss: 1.5852820873260498\n",
            "loss: 1.50970458984375\n",
            "loss: 1.6020736694335938\n",
            "loss: 1.5282275676727295\n",
            "loss: 1.6250896453857422\n",
            "loss: 1.5242946147918701\n",
            "loss: 1.513849139213562\n",
            "loss: 1.567231297492981\n",
            "loss: 1.5274759531021118\n",
            "loss: 1.532523512840271\n",
            "loss: 1.6344443559646606\n",
            "loss: 1.5495487451553345\n",
            "loss: 1.5234943628311157\n",
            "loss: 1.6068006753921509\n",
            "loss: 1.5982989072799683\n",
            "loss: 1.6044107675552368\n",
            "loss: 1.431035041809082\n",
            "loss: 1.4971246719360352\n",
            "loss: 1.5446449518203735\n",
            "loss: 1.5826283693313599\n",
            "loss: 1.5308208465576172\n",
            "loss: 1.5567511320114136\n",
            "loss: 1.5505633354187012\n",
            "loss: 1.5612488985061646\n",
            "loss: 1.5502588748931885\n",
            "loss: 1.5247371196746826\n",
            "loss: 1.5936975479125977\n",
            "loss: 1.564389944076538\n",
            "loss: 1.6352728605270386\n",
            "loss: 1.5265171527862549\n",
            "loss: 1.5168840885162354\n",
            "loss: 1.6233136653900146\n",
            "loss: 1.499407172203064\n",
            "loss: 1.6321656703948975\n",
            "loss: 1.6119548082351685\n",
            "loss: 1.6747233867645264\n",
            "loss: 1.593714714050293\n",
            "loss: 1.4790093898773193\n",
            "loss: 1.5230846405029297\n",
            "loss: 1.6560379266738892\n",
            "loss: 1.5555953979492188\n",
            "loss: 1.4750303030014038\n",
            "loss: 1.554490089416504\n",
            "loss: 1.5646464824676514\n",
            "loss: 1.4635169506072998\n",
            "loss: 1.5674669742584229\n",
            "loss: 1.454265832901001\n",
            "loss: 1.5853885412216187\n",
            "loss: 1.5765130519866943\n",
            "loss: 1.4777408838272095\n",
            "loss: 1.5047355890274048\n",
            "loss: 1.5353645086288452\n",
            "loss: 1.5521992444992065\n",
            "loss: 1.5542460680007935\n",
            "loss: 1.5808260440826416\n",
            "loss: 1.5821747779846191\n",
            "loss: 1.6007616519927979\n",
            "loss: 1.5482763051986694\n",
            "loss: 1.6175936460494995\n",
            "loss: 1.5301541090011597\n",
            "loss: 1.541104793548584\n",
            "loss: 1.5858405828475952\n",
            "loss: 1.5248596668243408\n",
            "loss: 1.542554259300232\n",
            "loss: 1.4672781229019165\n",
            "loss: 1.5301660299301147\n",
            "loss: 1.5846689939498901\n",
            "loss: 1.555182695388794\n",
            "loss: 1.499561071395874\n",
            "loss: 1.5876230001449585\n",
            "loss: 1.6231244802474976\n",
            "loss: 1.6221966743469238\n",
            "loss: 1.5721683502197266\n",
            "loss: 1.616111159324646\n",
            "loss: 1.528515100479126\n",
            "loss: 1.445067048072815\n",
            "loss: 1.632452368736267\n",
            "loss: 1.5792627334594727\n",
            "loss: 1.546643614768982\n",
            "loss: 1.5723274946212769\n",
            "loss: 1.5817545652389526\n",
            "loss: 1.4626530408859253\n",
            "loss: 1.6023367643356323\n",
            "loss: 1.5550422668457031\n",
            "loss: 1.6546815633773804\n",
            "loss: 1.5993382930755615\n",
            "loss: 1.6066243648529053\n",
            "loss: 1.5645990371704102\n",
            "loss: 1.5199609994888306\n",
            "loss: 1.5894196033477783\n",
            "loss: 1.5471714735031128\n",
            "loss: 1.5063515901565552\n",
            "loss: 1.514960765838623\n",
            "loss: 1.5889427661895752\n",
            "loss: 1.4622138738632202\n",
            "loss: 1.6737499237060547\n",
            "loss: 1.4749103784561157\n",
            "loss: 1.626173973083496\n",
            "loss: 1.5593186616897583\n",
            "loss: 1.5886858701705933\n",
            "loss: 1.524682879447937\n",
            "loss: 1.605104684829712\n",
            "loss: 1.584616780281067\n",
            "loss: 1.5268735885620117\n",
            "loss: 1.5253229141235352\n",
            "loss: 1.5726940631866455\n",
            "loss: 1.5209643840789795\n",
            "loss: 1.6536394357681274\n",
            "loss: 1.4704183340072632\n",
            "loss: 1.5581482648849487\n",
            "loss: 1.5799280405044556\n",
            "loss: 1.5354561805725098\n",
            "loss: 1.5170172452926636\n",
            "loss: 1.606637716293335\n",
            "loss: 1.5240468978881836\n",
            "loss: 1.6451640129089355\n",
            "loss: 1.50872802734375\n",
            "loss: 1.5370888710021973\n",
            "loss: 1.523840308189392\n",
            "loss: 1.4877198934555054\n",
            "loss: 1.5155633687973022\n",
            "loss: 1.5097652673721313\n",
            "loss: 1.5582647323608398\n",
            "loss: 1.4850006103515625\n",
            "loss: 1.631752848625183\n",
            "loss: 1.583214282989502\n",
            "loss: 1.5284757614135742\n",
            "loss: 1.4296412467956543\n",
            "loss: 1.5377087593078613\n",
            "loss: 1.598544716835022\n",
            "loss: 1.5511366128921509\n",
            "loss: 1.493054747581482\n",
            "loss: 1.5244015455245972\n",
            "loss: 1.5787783861160278\n",
            "loss: 1.586720585823059\n",
            "loss: 1.6205732822418213\n",
            "loss: 1.5274550914764404\n",
            "Epoch [4/10], Loss: 1.5275\n",
            "loss: 1.5690447092056274\n",
            "loss: 1.5885132551193237\n",
            "loss: 1.5752184391021729\n",
            "loss: 1.6019415855407715\n",
            "loss: 1.5233352184295654\n",
            "loss: 1.50870943069458\n",
            "loss: 1.5000418424606323\n",
            "loss: 1.6277031898498535\n",
            "loss: 1.5448282957077026\n",
            "loss: 1.6671193838119507\n",
            "loss: 1.5480601787567139\n",
            "loss: 1.4467355012893677\n",
            "loss: 1.5009584426879883\n",
            "loss: 1.4826548099517822\n",
            "loss: 1.6103559732437134\n",
            "loss: 1.5066330432891846\n",
            "loss: 1.4756561517715454\n",
            "loss: 1.5717904567718506\n",
            "loss: 1.6418356895446777\n",
            "loss: 1.610672116279602\n",
            "loss: 1.425605058670044\n",
            "loss: 1.4889299869537354\n",
            "loss: 1.488623023033142\n",
            "loss: 1.4991427659988403\n",
            "loss: 1.468481183052063\n",
            "loss: 1.4740281105041504\n",
            "loss: 1.4674872159957886\n",
            "loss: 1.4979814291000366\n",
            "loss: 1.4961731433868408\n",
            "loss: 1.6093844175338745\n",
            "loss: 1.5154722929000854\n",
            "loss: 1.6234444379806519\n",
            "loss: 1.5569372177124023\n",
            "loss: 1.5207862854003906\n",
            "loss: 1.569777250289917\n",
            "loss: 1.5533396005630493\n",
            "loss: 1.6022037267684937\n",
            "loss: 1.5283228158950806\n",
            "loss: 1.6124554872512817\n",
            "loss: 1.53792142868042\n",
            "loss: 1.4627251625061035\n",
            "loss: 1.4566065073013306\n",
            "loss: 1.528508186340332\n",
            "loss: 1.5277565717697144\n",
            "loss: 1.5274423360824585\n",
            "loss: 1.6290072202682495\n",
            "loss: 1.5445873737335205\n",
            "loss: 1.5866600275039673\n",
            "loss: 1.5461328029632568\n",
            "loss: 1.6481479406356812\n",
            "loss: 1.5591222047805786\n",
            "loss: 1.518814206123352\n",
            "loss: 1.5258289575576782\n",
            "loss: 1.5945950746536255\n",
            "loss: 1.5744962692260742\n",
            "loss: 1.4906576871871948\n",
            "loss: 1.5503112077713013\n",
            "loss: 1.5475600957870483\n",
            "loss: 1.4804835319519043\n",
            "loss: 1.5743138790130615\n",
            "loss: 1.5100395679473877\n",
            "loss: 1.4592888355255127\n",
            "loss: 1.5666906833648682\n",
            "loss: 1.5558384656906128\n",
            "loss: 1.5137019157409668\n",
            "loss: 1.5390467643737793\n",
            "loss: 1.5669372081756592\n",
            "loss: 1.4513335227966309\n",
            "loss: 1.5065689086914062\n",
            "loss: 1.4827206134796143\n",
            "loss: 1.459104061126709\n",
            "loss: 1.5660783052444458\n",
            "loss: 1.5314319133758545\n",
            "loss: 1.5316047668457031\n",
            "loss: 1.6074498891830444\n",
            "loss: 1.5684022903442383\n",
            "loss: 1.487982153892517\n",
            "loss: 1.4974361658096313\n",
            "loss: 1.5295708179473877\n",
            "loss: 1.5167981386184692\n",
            "loss: 1.6434706449508667\n",
            "loss: 1.5901890993118286\n",
            "loss: 1.55099356174469\n",
            "loss: 1.5662729740142822\n",
            "loss: 1.4655816555023193\n",
            "loss: 1.4878709316253662\n",
            "loss: 1.4738876819610596\n",
            "loss: 1.5798487663269043\n",
            "loss: 1.4421671628952026\n",
            "loss: 1.4034326076507568\n",
            "loss: 1.6050711870193481\n",
            "loss: 1.4651802778244019\n",
            "loss: 1.533695101737976\n",
            "loss: 1.5535752773284912\n",
            "loss: 1.5022200345993042\n",
            "loss: 1.512404441833496\n",
            "loss: 1.4993538856506348\n",
            "loss: 1.477965235710144\n",
            "loss: 1.468117356300354\n",
            "loss: 1.52869713306427\n",
            "loss: 1.4924079179763794\n",
            "loss: 1.535908818244934\n",
            "loss: 1.5853530168533325\n",
            "loss: 1.5562875270843506\n",
            "loss: 1.5727180242538452\n",
            "loss: 1.5437917709350586\n",
            "loss: 1.566631555557251\n",
            "loss: 1.490034580230713\n",
            "loss: 1.5285131931304932\n",
            "loss: 1.5564942359924316\n",
            "loss: 1.5336024761199951\n",
            "loss: 1.5988179445266724\n",
            "loss: 1.5325008630752563\n",
            "loss: 1.5721124410629272\n",
            "loss: 1.511277437210083\n",
            "loss: 1.5229861736297607\n",
            "loss: 1.5050052404403687\n",
            "loss: 1.5310823917388916\n",
            "loss: 1.6006319522857666\n",
            "loss: 1.4777896404266357\n",
            "loss: 1.490845799446106\n",
            "loss: 1.5567690134048462\n",
            "loss: 1.4444854259490967\n",
            "loss: 1.515563726425171\n",
            "loss: 1.503350853919983\n",
            "loss: 1.5331531763076782\n",
            "loss: 1.4117867946624756\n",
            "loss: 1.409767746925354\n",
            "loss: 1.5662885904312134\n",
            "loss: 1.521403193473816\n",
            "loss: 1.6192517280578613\n",
            "loss: 1.5945556163787842\n",
            "loss: 1.5891844034194946\n",
            "loss: 1.484297275543213\n",
            "loss: 1.5095316171646118\n",
            "loss: 1.5748562812805176\n",
            "loss: 1.5429131984710693\n",
            "loss: 1.4343286752700806\n",
            "loss: 1.572299838066101\n",
            "loss: 1.5799295902252197\n",
            "loss: 1.575524926185608\n",
            "loss: 1.5228179693222046\n",
            "loss: 1.5092030763626099\n",
            "loss: 1.489198923110962\n",
            "loss: 1.5428216457366943\n",
            "loss: 1.4934897422790527\n",
            "loss: 1.5175796747207642\n",
            "loss: 1.6455307006835938\n",
            "loss: 1.5301475524902344\n",
            "loss: 1.6765800714492798\n",
            "Epoch [5/10], Loss: 1.6766\n",
            "loss: 1.5517128705978394\n",
            "loss: 1.4487919807434082\n",
            "loss: 1.5743972063064575\n",
            "loss: 1.331683874130249\n",
            "loss: 1.471808910369873\n",
            "loss: 1.5244064331054688\n",
            "loss: 1.5231209993362427\n",
            "loss: 1.5823016166687012\n",
            "loss: 1.4290385246276855\n",
            "loss: 1.5302224159240723\n",
            "loss: 1.5404260158538818\n",
            "loss: 1.5546939373016357\n",
            "loss: 1.485081672668457\n",
            "loss: 1.5177295207977295\n",
            "loss: 1.548569917678833\n",
            "loss: 1.4534504413604736\n",
            "loss: 1.5053327083587646\n",
            "loss: 1.4751118421554565\n",
            "loss: 1.509401798248291\n",
            "loss: 1.496767282485962\n",
            "loss: 1.558748483657837\n",
            "loss: 1.4613221883773804\n",
            "loss: 1.5067648887634277\n",
            "loss: 1.6341816186904907\n",
            "loss: 1.5678520202636719\n",
            "loss: 1.5809062719345093\n",
            "loss: 1.5470318794250488\n",
            "loss: 1.4731097221374512\n",
            "loss: 1.5314265489578247\n",
            "loss: 1.4697059392929077\n",
            "loss: 1.6045689582824707\n",
            "loss: 1.5981796979904175\n",
            "loss: 1.6549252271652222\n",
            "loss: 1.5171844959259033\n",
            "loss: 1.5712190866470337\n",
            "loss: 1.4776535034179688\n",
            "loss: 1.4376534223556519\n",
            "loss: 1.457465648651123\n",
            "loss: 1.4108035564422607\n",
            "loss: 1.5002894401550293\n",
            "loss: 1.502001166343689\n",
            "loss: 1.4731166362762451\n",
            "loss: 1.58425772190094\n",
            "loss: 1.5298919677734375\n",
            "loss: 1.5067071914672852\n",
            "loss: 1.493464469909668\n",
            "loss: 1.4005742073059082\n",
            "loss: 1.5491119623184204\n",
            "loss: 1.5388994216918945\n",
            "loss: 1.5296272039413452\n",
            "loss: 1.5100219249725342\n",
            "loss: 1.4897642135620117\n",
            "loss: 1.5882128477096558\n",
            "loss: 1.5607281923294067\n",
            "loss: 1.5951616764068604\n",
            "loss: 1.5335694551467896\n",
            "loss: 1.3988038301467896\n",
            "loss: 1.4533977508544922\n",
            "loss: 1.4767882823944092\n",
            "loss: 1.513211965560913\n",
            "loss: 1.429613709449768\n",
            "loss: 1.5569632053375244\n",
            "loss: 1.555698275566101\n",
            "loss: 1.5532946586608887\n",
            "loss: 1.4368679523468018\n",
            "loss: 1.4235916137695312\n",
            "loss: 1.4815127849578857\n",
            "loss: 1.503611445426941\n",
            "loss: 1.5523039102554321\n",
            "loss: 1.4716432094573975\n",
            "loss: 1.4599583148956299\n",
            "loss: 1.4049732685089111\n",
            "loss: 1.4541189670562744\n",
            "loss: 1.5470865964889526\n",
            "loss: 1.4411803483963013\n",
            "loss: 1.4796974658966064\n",
            "loss: 1.466524600982666\n",
            "loss: 1.467355728149414\n",
            "loss: 1.6067826747894287\n",
            "loss: 1.6755235195159912\n",
            "loss: 1.5603803396224976\n",
            "loss: 1.4440314769744873\n",
            "loss: 1.5218989849090576\n",
            "loss: 1.5406469106674194\n",
            "loss: 1.4893776178359985\n",
            "loss: 1.4724540710449219\n",
            "loss: 1.4946571588516235\n",
            "loss: 1.563847303390503\n",
            "loss: 1.5324478149414062\n",
            "loss: 1.478752851486206\n",
            "loss: 1.5374629497528076\n",
            "loss: 1.5010839700698853\n",
            "loss: 1.5045228004455566\n",
            "loss: 1.4697551727294922\n",
            "loss: 1.5885567665100098\n",
            "loss: 1.546052098274231\n",
            "loss: 1.5171085596084595\n",
            "loss: 1.6137853860855103\n",
            "loss: 1.508939266204834\n",
            "loss: 1.5050853490829468\n",
            "loss: 1.500479817390442\n",
            "loss: 1.567677617073059\n",
            "loss: 1.5736416578292847\n",
            "loss: 1.4560829401016235\n",
            "loss: 1.4261562824249268\n",
            "loss: 1.5144085884094238\n",
            "loss: 1.5038795471191406\n",
            "loss: 1.5160788297653198\n",
            "loss: 1.4836076498031616\n",
            "loss: 1.4546165466308594\n",
            "loss: 1.440852403640747\n",
            "loss: 1.478793740272522\n",
            "loss: 1.5067788362503052\n",
            "loss: 1.5100417137145996\n",
            "loss: 1.505099892616272\n",
            "loss: 1.4974853992462158\n",
            "loss: 1.4777474403381348\n",
            "loss: 1.4293681383132935\n",
            "loss: 1.5793938636779785\n",
            "loss: 1.4364221096038818\n",
            "loss: 1.4279029369354248\n",
            "loss: 1.5283868312835693\n",
            "loss: 1.460571050643921\n",
            "loss: 1.53053617477417\n",
            "loss: 1.5283753871917725\n",
            "loss: 1.5450963973999023\n",
            "loss: 1.4865213632583618\n",
            "loss: 1.4701268672943115\n",
            "loss: 1.536674976348877\n",
            "loss: 1.5297133922576904\n",
            "loss: 1.5475757122039795\n",
            "loss: 1.4904987812042236\n",
            "loss: 1.551657795906067\n",
            "loss: 1.5175251960754395\n",
            "loss: 1.5149842500686646\n",
            "loss: 1.5666803121566772\n",
            "loss: 1.6714364290237427\n",
            "loss: 1.4548799991607666\n",
            "loss: 1.502236247062683\n",
            "loss: 1.4537572860717773\n",
            "loss: 1.6154992580413818\n",
            "loss: 1.5261824131011963\n",
            "loss: 1.5490602254867554\n",
            "loss: 1.5235999822616577\n",
            "loss: 1.6063951253890991\n",
            "loss: 1.5670212507247925\n",
            "loss: 1.6004115343093872\n",
            "loss: 1.5025883913040161\n",
            "loss: 1.4997386932373047\n",
            "loss: 1.572529673576355\n",
            "Epoch [6/10], Loss: 1.5725\n",
            "loss: 1.4976283311843872\n",
            "loss: 1.638211727142334\n",
            "loss: 1.5223844051361084\n",
            "loss: 1.4352902173995972\n",
            "loss: 1.5888283252716064\n",
            "loss: 1.617101788520813\n",
            "loss: 1.5374385118484497\n",
            "loss: 1.427733063697815\n",
            "loss: 1.5730761289596558\n",
            "loss: 1.5277812480926514\n",
            "loss: 1.618071436882019\n",
            "loss: 1.6101880073547363\n",
            "loss: 1.4031184911727905\n",
            "loss: 1.5422234535217285\n",
            "loss: 1.4934401512145996\n",
            "loss: 1.4925923347473145\n",
            "loss: 1.5638022422790527\n",
            "loss: 1.4480174779891968\n",
            "loss: 1.517957329750061\n",
            "loss: 1.4723767042160034\n",
            "loss: 1.5523886680603027\n",
            "loss: 1.4180386066436768\n",
            "loss: 1.5244888067245483\n",
            "loss: 1.4826074838638306\n",
            "loss: 1.5075085163116455\n",
            "loss: 1.5850703716278076\n",
            "loss: 1.4243069887161255\n",
            "loss: 1.5770267248153687\n",
            "loss: 1.6203325986862183\n",
            "loss: 1.5552922487258911\n",
            "loss: 1.4983737468719482\n",
            "loss: 1.511291265487671\n",
            "loss: 1.577351450920105\n",
            "loss: 1.5155633687973022\n",
            "loss: 1.4935176372528076\n",
            "loss: 1.5563428401947021\n",
            "loss: 1.484710931777954\n",
            "loss: 1.5576518774032593\n",
            "loss: 1.5220969915390015\n",
            "loss: 1.5263139009475708\n",
            "loss: 1.5294016599655151\n",
            "loss: 1.548392415046692\n",
            "loss: 1.554479956626892\n",
            "loss: 1.4905517101287842\n",
            "loss: 1.4976520538330078\n",
            "loss: 1.497968077659607\n",
            "loss: 1.51054847240448\n",
            "loss: 1.4858262538909912\n",
            "loss: 1.4084134101867676\n",
            "loss: 1.5057146549224854\n",
            "loss: 1.4990371465682983\n",
            "loss: 1.4598238468170166\n",
            "loss: 1.4925017356872559\n",
            "loss: 1.5671507120132446\n",
            "loss: 1.6393849849700928\n",
            "loss: 1.4978753328323364\n",
            "loss: 1.5155982971191406\n",
            "loss: 1.5162904262542725\n",
            "loss: 1.497625470161438\n",
            "loss: 1.635348916053772\n",
            "loss: 1.537921667098999\n",
            "loss: 1.4451007843017578\n",
            "loss: 1.4825823307037354\n",
            "loss: 1.5561569929122925\n",
            "loss: 1.5042951107025146\n",
            "loss: 1.5212868452072144\n",
            "loss: 1.4408260583877563\n",
            "loss: 1.4665614366531372\n",
            "loss: 1.466098666191101\n",
            "loss: 1.5166724920272827\n",
            "loss: 1.5862098932266235\n",
            "loss: 1.5544508695602417\n",
            "loss: 1.545449137687683\n",
            "loss: 1.5246824026107788\n",
            "loss: 1.4562313556671143\n",
            "loss: 1.4594321250915527\n",
            "loss: 1.5141271352767944\n",
            "loss: 1.4431482553482056\n",
            "loss: 1.5258888006210327\n",
            "loss: 1.529279351234436\n",
            "loss: 1.5196220874786377\n",
            "loss: 1.584599256515503\n",
            "loss: 1.471891164779663\n",
            "loss: 1.541513442993164\n",
            "loss: 1.5836660861968994\n",
            "loss: 1.4475964307785034\n",
            "loss: 1.5874354839324951\n",
            "loss: 1.510702133178711\n",
            "loss: 1.3972982168197632\n",
            "loss: 1.644808292388916\n",
            "loss: 1.488765001296997\n",
            "loss: 1.4311169385910034\n",
            "loss: 1.5060181617736816\n",
            "loss: 1.3864057064056396\n",
            "loss: 1.5159426927566528\n",
            "loss: 1.5586400032043457\n",
            "loss: 1.5064946413040161\n",
            "loss: 1.602123737335205\n",
            "loss: 1.5816909074783325\n",
            "loss: 1.4703407287597656\n",
            "loss: 1.5293992757797241\n",
            "loss: 1.4923015832901\n",
            "loss: 1.4895243644714355\n",
            "loss: 1.4305974245071411\n",
            "loss: 1.4877437353134155\n",
            "loss: 1.5963202714920044\n",
            "loss: 1.5409533977508545\n",
            "loss: 1.588757872581482\n",
            "loss: 1.5665500164031982\n",
            "loss: 1.501176118850708\n",
            "loss: 1.5023653507232666\n",
            "loss: 1.3581498861312866\n",
            "loss: 1.4532568454742432\n",
            "loss: 1.4313936233520508\n",
            "loss: 1.4625823497772217\n",
            "loss: 1.4622323513031006\n",
            "loss: 1.4567474126815796\n",
            "loss: 1.5051032304763794\n",
            "loss: 1.4780306816101074\n",
            "loss: 1.464967966079712\n",
            "loss: 1.4416065216064453\n",
            "loss: 1.524451732635498\n",
            "loss: 1.4848031997680664\n",
            "loss: 1.503402829170227\n",
            "loss: 1.4533997774124146\n",
            "loss: 1.3851526975631714\n",
            "loss: 1.472591519355774\n",
            "loss: 1.4601095914840698\n",
            "loss: 1.3842254877090454\n",
            "loss: 1.4807440042495728\n",
            "loss: 1.4458609819412231\n",
            "loss: 1.4133336544036865\n",
            "loss: 1.4359525442123413\n",
            "loss: 1.4831678867340088\n",
            "loss: 1.490969181060791\n",
            "loss: 1.5187925100326538\n",
            "loss: 1.4708455801010132\n",
            "loss: 1.5679583549499512\n",
            "loss: 1.4236851930618286\n",
            "loss: 1.5710381269454956\n",
            "loss: 1.4514695405960083\n",
            "loss: 1.5733641386032104\n",
            "loss: 1.418062686920166\n",
            "loss: 1.6283833980560303\n",
            "loss: 1.5051299333572388\n",
            "loss: 1.546134114265442\n",
            "loss: 1.5579620599746704\n",
            "loss: 1.5568547248840332\n",
            "loss: 1.528417706489563\n",
            "loss: 1.4825825691223145\n",
            "Epoch [7/10], Loss: 1.4826\n",
            "loss: 1.501206398010254\n",
            "loss: 1.449444055557251\n",
            "loss: 1.4522417783737183\n",
            "loss: 1.5310206413269043\n",
            "loss: 1.44808030128479\n",
            "loss: 1.37674880027771\n",
            "loss: 1.5225677490234375\n",
            "loss: 1.496080756187439\n",
            "loss: 1.4970983266830444\n",
            "loss: 1.5495566129684448\n",
            "loss: 1.4541889429092407\n",
            "loss: 1.4466737508773804\n",
            "loss: 1.4177967309951782\n",
            "loss: 1.4553451538085938\n",
            "loss: 1.5320968627929688\n",
            "loss: 1.6103476285934448\n",
            "loss: 1.4452134370803833\n",
            "loss: 1.4896401166915894\n",
            "loss: 1.559406042098999\n",
            "loss: 1.5034606456756592\n",
            "loss: 1.4524643421173096\n",
            "loss: 1.5075184106826782\n",
            "loss: 1.545624017715454\n",
            "loss: 1.449129581451416\n",
            "loss: 1.485097050666809\n",
            "loss: 1.5422015190124512\n",
            "loss: 1.4297511577606201\n",
            "loss: 1.569437861442566\n",
            "loss: 1.5259411334991455\n",
            "loss: 1.5767203569412231\n",
            "loss: 1.489444375038147\n",
            "loss: 1.563633680343628\n",
            "loss: 1.5118558406829834\n",
            "loss: 1.5903452634811401\n",
            "loss: 1.518273949623108\n",
            "loss: 1.5210628509521484\n",
            "loss: 1.5082876682281494\n",
            "loss: 1.4858567714691162\n",
            "loss: 1.5390955209732056\n",
            "loss: 1.4957482814788818\n",
            "loss: 1.5525590181350708\n",
            "loss: 1.4487532377243042\n",
            "loss: 1.544499397277832\n",
            "loss: 1.448477864265442\n",
            "loss: 1.4018902778625488\n",
            "loss: 1.5206784009933472\n",
            "loss: 1.53001868724823\n",
            "loss: 1.5128117799758911\n",
            "loss: 1.591868281364441\n",
            "loss: 1.4934821128845215\n",
            "loss: 1.516526460647583\n",
            "loss: 1.4841973781585693\n",
            "loss: 1.431060552597046\n",
            "loss: 1.5274736881256104\n",
            "loss: 1.4329015016555786\n",
            "loss: 1.5248730182647705\n",
            "loss: 1.4392706155776978\n",
            "loss: 1.5481998920440674\n",
            "loss: 1.5014194250106812\n",
            "loss: 1.4630815982818604\n",
            "loss: 1.461143970489502\n",
            "loss: 1.4333431720733643\n",
            "loss: 1.5018311738967896\n",
            "loss: 1.5070407390594482\n",
            "loss: 1.460817813873291\n",
            "loss: 1.5151642560958862\n",
            "loss: 1.4021039009094238\n",
            "loss: 1.4991347789764404\n",
            "loss: 1.5013165473937988\n",
            "loss: 1.4273377656936646\n",
            "loss: 1.480830430984497\n",
            "loss: 1.5242120027542114\n",
            "loss: 1.5414538383483887\n",
            "loss: 1.5048831701278687\n",
            "loss: 1.444941759109497\n",
            "loss: 1.5077208280563354\n",
            "loss: 1.483052372932434\n",
            "loss: 1.5488108396530151\n",
            "loss: 1.4103487730026245\n",
            "loss: 1.552101492881775\n",
            "loss: 1.4921362400054932\n",
            "loss: 1.4465633630752563\n",
            "loss: 1.5151423215866089\n",
            "loss: 1.4912724494934082\n",
            "loss: 1.4459127187728882\n",
            "loss: 1.455317497253418\n",
            "loss: 1.5006132125854492\n",
            "loss: 1.5548269748687744\n",
            "loss: 1.5187182426452637\n",
            "loss: 1.5320703983306885\n",
            "loss: 1.4616395235061646\n",
            "loss: 1.5060439109802246\n",
            "loss: 1.5395334959030151\n",
            "loss: 1.4938626289367676\n",
            "loss: 1.4518134593963623\n",
            "loss: 1.5689685344696045\n",
            "loss: 1.4580501317977905\n",
            "loss: 1.4228802919387817\n",
            "loss: 1.5081837177276611\n",
            "loss: 1.5024514198303223\n",
            "loss: 1.464343547821045\n",
            "loss: 1.5093246698379517\n",
            "loss: 1.5195657014846802\n",
            "loss: 1.4967665672302246\n",
            "loss: 1.4653092622756958\n",
            "loss: 1.5610487461090088\n",
            "loss: 1.4485340118408203\n",
            "loss: 1.3972140550613403\n",
            "loss: 1.4990068674087524\n",
            "loss: 1.539080262184143\n",
            "loss: 1.3983900547027588\n",
            "loss: 1.5435634851455688\n",
            "loss: 1.481308937072754\n",
            "loss: 1.4836070537567139\n",
            "loss: 1.3767151832580566\n",
            "loss: 1.4664345979690552\n",
            "loss: 1.5431181192398071\n",
            "loss: 1.4700170755386353\n",
            "loss: 1.5148460865020752\n",
            "loss: 1.4929088354110718\n",
            "loss: 1.400235891342163\n",
            "loss: 1.4301958084106445\n",
            "loss: 1.438151478767395\n",
            "loss: 1.4445537328720093\n",
            "loss: 1.4898918867111206\n",
            "loss: 1.6205559968948364\n",
            "loss: 1.4878648519515991\n",
            "loss: 1.5090444087982178\n",
            "loss: 1.5844193696975708\n",
            "loss: 1.536176323890686\n",
            "loss: 1.3738199472427368\n",
            "loss: 1.4518928527832031\n",
            "loss: 1.4642987251281738\n",
            "loss: 1.4457709789276123\n",
            "loss: 1.4104422330856323\n",
            "loss: 1.4623647928237915\n",
            "loss: 1.398502230644226\n",
            "loss: 1.4992529153823853\n",
            "loss: 1.5116033554077148\n",
            "loss: 1.474011778831482\n",
            "loss: 1.4887800216674805\n",
            "loss: 1.5406115055084229\n",
            "loss: 1.5320191383361816\n",
            "loss: 1.4832875728607178\n",
            "loss: 1.46728515625\n",
            "loss: 1.6061526536941528\n",
            "loss: 1.4413559436798096\n",
            "loss: 1.5578734874725342\n",
            "loss: 1.5367515087127686\n",
            "loss: 1.4996076822280884\n",
            "Epoch [8/10], Loss: 1.4996\n",
            "loss: 1.5274678468704224\n",
            "loss: 1.406009316444397\n",
            "loss: 1.4287019968032837\n",
            "loss: 1.5139186382293701\n",
            "loss: 1.4778167009353638\n",
            "loss: 1.5457863807678223\n",
            "loss: 1.513750433921814\n",
            "loss: 1.5560498237609863\n",
            "loss: 1.4808496236801147\n",
            "loss: 1.4289849996566772\n",
            "loss: 1.4949018955230713\n",
            "loss: 1.4642703533172607\n",
            "loss: 1.501780390739441\n",
            "loss: 1.4975848197937012\n",
            "loss: 1.5570063591003418\n",
            "loss: 1.4400748014450073\n",
            "loss: 1.5595052242279053\n",
            "loss: 1.4430814981460571\n",
            "loss: 1.3703837394714355\n",
            "loss: 1.5668838024139404\n",
            "loss: 1.5583027601242065\n",
            "loss: 1.4991225004196167\n",
            "loss: 1.483581781387329\n",
            "loss: 1.4505395889282227\n",
            "loss: 1.5178802013397217\n",
            "loss: 1.6020375490188599\n",
            "loss: 1.446839451789856\n",
            "loss: 1.4730080366134644\n",
            "loss: 1.5292600393295288\n",
            "loss: 1.4902533292770386\n",
            "loss: 1.5285978317260742\n",
            "loss: 1.4269391298294067\n",
            "loss: 1.4730712175369263\n",
            "loss: 1.476022481918335\n",
            "loss: 1.5971845388412476\n",
            "loss: 1.525004506111145\n",
            "loss: 1.4549506902694702\n",
            "loss: 1.4758355617523193\n",
            "loss: 1.4050263166427612\n",
            "loss: 1.4950400590896606\n",
            "loss: 1.4307867288589478\n",
            "loss: 1.3727859258651733\n",
            "loss: 1.4494272470474243\n",
            "loss: 1.5402876138687134\n",
            "loss: 1.3887643814086914\n",
            "loss: 1.518803596496582\n",
            "loss: 1.477951169013977\n",
            "loss: 1.4464681148529053\n",
            "loss: 1.4518126249313354\n",
            "loss: 1.3972136974334717\n",
            "loss: 1.3977479934692383\n",
            "loss: 1.5586904287338257\n",
            "loss: 1.3757665157318115\n",
            "loss: 1.4789458513259888\n",
            "loss: 1.3703408241271973\n",
            "loss: 1.3761337995529175\n",
            "loss: 1.445727825164795\n",
            "loss: 1.4487789869308472\n",
            "loss: 1.4243170022964478\n",
            "loss: 1.4335896968841553\n",
            "loss: 1.434609055519104\n",
            "loss: 1.3836888074874878\n",
            "loss: 1.4914581775665283\n",
            "loss: 1.5377994775772095\n",
            "loss: 1.5360982418060303\n",
            "loss: 1.4579397439956665\n",
            "loss: 1.4998890161514282\n",
            "loss: 1.3577580451965332\n",
            "loss: 1.4704149961471558\n",
            "loss: 1.4264408349990845\n",
            "loss: 1.5051071643829346\n",
            "loss: 1.5801911354064941\n",
            "loss: 1.499394178390503\n",
            "loss: 1.4808580875396729\n",
            "loss: 1.3699414730072021\n",
            "loss: 1.4559892416000366\n",
            "loss: 1.4167671203613281\n",
            "loss: 1.493862271308899\n",
            "loss: 1.4115887880325317\n",
            "loss: 1.455960988998413\n",
            "loss: 1.4502856731414795\n",
            "loss: 1.3934158086776733\n",
            "loss: 1.3976855278015137\n",
            "loss: 1.4575551748275757\n",
            "loss: 1.4454798698425293\n",
            "loss: 1.4582403898239136\n",
            "loss: 1.5732990503311157\n",
            "loss: 1.4723907709121704\n",
            "loss: 1.4458155632019043\n",
            "loss: 1.4381544589996338\n",
            "loss: 1.5023952722549438\n",
            "loss: 1.3763930797576904\n",
            "loss: 1.4734033346176147\n",
            "loss: 1.5855515003204346\n",
            "loss: 1.4513264894485474\n",
            "loss: 1.5108064413070679\n",
            "loss: 1.587351679801941\n",
            "loss: 1.4699007272720337\n",
            "loss: 1.5263224840164185\n",
            "loss: 1.4817768335342407\n",
            "loss: 1.4889122247695923\n",
            "loss: 1.456133484840393\n",
            "loss: 1.5455981492996216\n",
            "loss: 1.4839847087860107\n",
            "loss: 1.4435254335403442\n",
            "loss: 1.4860185384750366\n",
            "loss: 1.5336511135101318\n",
            "loss: 1.512442708015442\n",
            "loss: 1.5127755403518677\n",
            "loss: 1.5379369258880615\n",
            "loss: 1.48789381980896\n",
            "loss: 1.4264718294143677\n",
            "loss: 1.5335192680358887\n",
            "loss: 1.3514882326126099\n",
            "loss: 1.4615545272827148\n",
            "loss: 1.4755233526229858\n",
            "loss: 1.4576051235198975\n",
            "loss: 1.4714611768722534\n",
            "loss: 1.466243028640747\n",
            "loss: 1.6362510919570923\n",
            "loss: 1.5213475227355957\n",
            "loss: 1.4963030815124512\n",
            "loss: 1.509161353111267\n",
            "loss: 1.4897760152816772\n",
            "loss: 1.4337598085403442\n",
            "loss: 1.4522913694381714\n",
            "loss: 1.5939744710922241\n",
            "loss: 1.4000669717788696\n",
            "loss: 1.5043390989303589\n",
            "loss: 1.5502930879592896\n",
            "loss: 1.4200325012207031\n",
            "loss: 1.492080807685852\n",
            "loss: 1.4804435968399048\n",
            "loss: 1.5698530673980713\n",
            "loss: 1.4533531665802002\n",
            "loss: 1.4835635423660278\n",
            "loss: 1.534541130065918\n",
            "loss: 1.4789127111434937\n",
            "loss: 1.5000591278076172\n",
            "loss: 1.4074373245239258\n",
            "loss: 1.4568146467208862\n",
            "loss: 1.5698683261871338\n",
            "loss: 1.4741004705429077\n",
            "loss: 1.3837231397628784\n",
            "loss: 1.4966648817062378\n",
            "loss: 1.420654535293579\n",
            "loss: 1.4924402236938477\n",
            "loss: 1.5906546115875244\n",
            "loss: 1.4239680767059326\n",
            "loss: 1.5459959506988525\n",
            "Epoch [9/10], Loss: 1.5460\n",
            "loss: 1.3793779611587524\n",
            "loss: 1.4591442346572876\n",
            "loss: 1.5105669498443604\n",
            "loss: 1.433716893196106\n",
            "loss: 1.413507103919983\n",
            "loss: 1.3843278884887695\n",
            "loss: 1.3353705406188965\n",
            "loss: 1.4230973720550537\n",
            "loss: 1.4772592782974243\n",
            "loss: 1.4251658916473389\n",
            "loss: 1.3730984926223755\n",
            "loss: 1.4180643558502197\n",
            "loss: 1.3515692949295044\n",
            "loss: 1.4292892217636108\n",
            "loss: 1.4739526510238647\n",
            "loss: 1.5187642574310303\n",
            "loss: 1.4495229721069336\n",
            "loss: 1.4178242683410645\n",
            "loss: 1.3810949325561523\n",
            "loss: 1.5088565349578857\n",
            "loss: 1.6055551767349243\n",
            "loss: 1.5579919815063477\n",
            "loss: 1.4888917207717896\n",
            "loss: 1.5217578411102295\n",
            "loss: 1.5355511903762817\n",
            "loss: 1.451235055923462\n",
            "loss: 1.4351156949996948\n",
            "loss: 1.4020066261291504\n",
            "loss: 1.4110785722732544\n",
            "loss: 1.4366670846939087\n",
            "loss: 1.4631972312927246\n",
            "loss: 1.4965713024139404\n",
            "loss: 1.492504596710205\n",
            "loss: 1.5516000986099243\n",
            "loss: 1.5545318126678467\n",
            "loss: 1.550961971282959\n",
            "loss: 1.6426825523376465\n",
            "loss: 1.4491795301437378\n",
            "loss: 1.455291986465454\n",
            "loss: 1.527532935142517\n",
            "loss: 1.3151848316192627\n",
            "loss: 1.436814785003662\n",
            "loss: 1.5260536670684814\n",
            "loss: 1.552262544631958\n",
            "loss: 1.4448593854904175\n",
            "loss: 1.5130839347839355\n",
            "loss: 1.4628560543060303\n",
            "loss: 1.4784948825836182\n",
            "loss: 1.47203528881073\n",
            "loss: 1.5157853364944458\n",
            "loss: 1.4411259889602661\n",
            "loss: 1.4138126373291016\n",
            "loss: 1.4336804151535034\n",
            "loss: 1.5148346424102783\n",
            "loss: 1.459031581878662\n",
            "loss: 1.4436885118484497\n",
            "loss: 1.5216773748397827\n",
            "loss: 1.4744927883148193\n",
            "loss: 1.3999394178390503\n",
            "loss: 1.4521362781524658\n",
            "loss: 1.4484081268310547\n",
            "loss: 1.4467684030532837\n",
            "loss: 1.3303321599960327\n",
            "loss: 1.490159273147583\n",
            "loss: 1.4284095764160156\n",
            "loss: 1.4602117538452148\n",
            "loss: 1.595881462097168\n",
            "loss: 1.4278550148010254\n",
            "loss: 1.4491323232650757\n",
            "loss: 1.4809529781341553\n",
            "loss: 1.4640153646469116\n",
            "loss: 1.3916715383529663\n",
            "loss: 1.3261189460754395\n",
            "loss: 1.4963852167129517\n",
            "loss: 1.4748164415359497\n",
            "loss: 1.4540672302246094\n",
            "loss: 1.4820313453674316\n",
            "loss: 1.5012307167053223\n",
            "loss: 1.567251205444336\n",
            "loss: 1.4669275283813477\n",
            "loss: 1.4826853275299072\n",
            "loss: 1.494502305984497\n",
            "loss: 1.4277101755142212\n",
            "loss: 1.5303163528442383\n",
            "loss: 1.5367528200149536\n",
            "loss: 1.5442827939987183\n",
            "loss: 1.5166680812835693\n",
            "loss: 1.5603440999984741\n",
            "loss: 1.3859978914260864\n",
            "loss: 1.4352476596832275\n",
            "loss: 1.5274405479431152\n",
            "loss: 1.4435120820999146\n",
            "loss: 1.4770334959030151\n",
            "loss: 1.507381558418274\n",
            "loss: 1.432137131690979\n",
            "loss: 1.456056833267212\n",
            "loss: 1.3657442331314087\n",
            "loss: 1.3918745517730713\n",
            "loss: 1.4788771867752075\n",
            "loss: 1.4883239269256592\n",
            "loss: 1.439807653427124\n",
            "loss: 1.6131728887557983\n",
            "loss: 1.4903156757354736\n",
            "loss: 1.4120713472366333\n",
            "loss: 1.4435138702392578\n",
            "loss: 1.4236624240875244\n",
            "loss: 1.4512720108032227\n",
            "loss: 1.5442651510238647\n",
            "loss: 1.5927201509475708\n",
            "loss: 1.4124679565429688\n",
            "loss: 1.461011290550232\n",
            "loss: 1.4407954216003418\n",
            "loss: 1.3571608066558838\n",
            "loss: 1.4696487188339233\n",
            "loss: 1.4740688800811768\n",
            "loss: 1.4841731786727905\n",
            "loss: 1.5366768836975098\n",
            "loss: 1.5129828453063965\n",
            "loss: 1.3916462659835815\n",
            "loss: 1.407945156097412\n",
            "loss: 1.4184335470199585\n",
            "loss: 1.3910181522369385\n",
            "loss: 1.4014095067977905\n",
            "loss: 1.4435806274414062\n",
            "loss: 1.3902642726898193\n",
            "loss: 1.3668116331100464\n",
            "loss: 1.4330997467041016\n",
            "loss: 1.5449965000152588\n",
            "loss: 1.5069156885147095\n",
            "loss: 1.5316121578216553\n",
            "loss: 1.4674756526947021\n",
            "loss: 1.4879512786865234\n",
            "loss: 1.470460057258606\n",
            "loss: 1.3485398292541504\n",
            "loss: 1.4639347791671753\n",
            "loss: 1.4038413763046265\n",
            "loss: 1.451293706893921\n",
            "loss: 1.4480012655258179\n",
            "loss: 1.5084627866744995\n",
            "loss: 1.487113118171692\n",
            "loss: 1.4750795364379883\n",
            "loss: 1.4638338088989258\n",
            "loss: 1.4849491119384766\n",
            "loss: 1.501781702041626\n",
            "loss: 1.372298240661621\n",
            "loss: 1.5839040279388428\n",
            "loss: 1.4687449932098389\n",
            "loss: 1.432946801185608\n",
            "loss: 1.511107325553894\n",
            "loss: 1.4413752555847168\n",
            "Epoch [10/10], Loss: 1.4414\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = lossFunction(outputs, labels)\n",
        "        print(f\"loss: {loss}\")\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LATYSdG-DFn4"
      },
      "source": [
        "## Prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0umi3jenDFMo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 9590 train images: 58.66527632950991 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the {train_dataset.__len__()} train images: {100 * correct / total} %')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6tDrTOMfUj3y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 5589 test images: 56.64698514940061 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the {test_dataset.__len__()} test images: {100 * correct / total} %')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsgxsieYUkuu"
      },
      "source": [
        "## Save to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save as torchScript model\n",
        "model.eval()\n",
        "model_scripted = torch.jit.script(model)\n",
        "model_scripted.save(r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\models\\\\torchScript200epoch.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save full model\n",
        "torch.save(model, r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\models\\\\200epoch.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wJPZr32HDLqy"
      },
      "outputs": [],
      "source": [
        "#Save model state dictionary\n",
        "\n",
        "torch.save(model.state_dict(), r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\models\\\\stateDict200epoch.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save LabelDecoder\n",
        "import pickle\n",
        "labelDecoder = train_dataset.getCharacterInverseTransform()\n",
        "with open(r'C:\\\\Users\\\\Katana GF66 11UC\\\\Documents\\\\GenshinImageClassifier\\\\models\\\\labelDecoder.pkl', 'wb') as f:\n",
        "    pickle.dump(labelDecoder, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Share to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hugging face PretrainedModel custom class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import PreTrainedModel\n",
        "\n",
        "class GenshinClassifier(PreTrainedModel):\n",
        "    def __init\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GNuhgUNObZA"
      },
      "source": [
        "# Open from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hZG5gvHOebg"
      },
      "outputs": [],
      "source": [
        "# prompt: open and make inference from  model.pt\n",
        "\n",
        "# Import the necessary libraries\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the model\n",
        "model = ConvNet(num_classes)\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "# Define the data transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the data\n",
        "data_dir = 'path/to/data'\n",
        "dataset = ImageFolder(data_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Make inference\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader:\n",
        "        # Move the images to the device\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Get the model's predictions\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get the predicted class labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Print the predicted class labels\n",
        "        print(predicted)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
